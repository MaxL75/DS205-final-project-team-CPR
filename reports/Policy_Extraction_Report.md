# Policy_Extraction_Report.md

## `climate_tracker/scripts/06_policy_extraction.py`


**Script Overview: 06\_policy\_extraction.py**

This script is the second stage of a policy analysis pipeline designed to extract structured, interpretable answers from retrieved climate policy evidence. It takes as input the country-level Markdown files generated by `05_information_retrieval.py` and produces a json file ` output/policies_targets_output.json` and produces per-country Markdown summaries in `policy_targets_pages/`. These outputs answer five predefined policy questions with structured fields.

---

**Objective**

This script addresses Sylvan's directive to *"improve the relevance of the results"* by:

* Breaking down complex policy questions into component parts
* Using enhanced NLP techniques to extract structured, explainable answers
* Outputting:

  * `yes_no` answers
  * standardised explanations
  * the original quote
  * confidence scores
  * and source metadata

---

**Core NLP Pipeline & Design Rationale**

We designed a layered pipeline to move from noisy text to structured understanding:

**Step 1: Enhanced NER Tagging**

* Uses regex + SpaCy NER to detect:

  * Years (`YEAR`) via explicit years, phrases like "by 2060", or even "mid-century"
  * Sectors (`SECTOR`) via keywords: electricity, agriculture, etc.
  * Policy types (`POLICY_TYPE`) and targets (`TARGET`) based on presence of legal terms or phrases like "net zero"

Used Regex because...
* It is used to find specific textual patterns in messy, natural language — especially when named entity recognition (NER) may not catch everything.
* It is precise and works well for consistent formats (e.g., dates), and complements SpaCy, which might miss less common phrasings or fail to extract context like "mid-century". 

Used SpaCy because...
* It provides language understanding beyond string-matching
* Dependency parsing is critical for capturing who is doing what and for detecting negation
* It helps make rules more robust to sentence variation

Together using Regex and Spacy allows you to use exact formats (e.g. "2030") - Regex, semantics (e.g. negation) - SpaCy, policy-specific terms - both, mutli-word expressions - SpaCy, fuzzy patterns or edge cases - Regex


**Step 2: Dependency Parsing**

* Extracts negations and underlying grammatical relationships (e.g., subject-verb-object structure)
* Example: Detects negated targets via dependency labels (e.g., `neg`), but does not override the answer. 


**Step 3: Semantic Scoring (Cosine Similarity)**

* Embeds the original question and retrieved chunk using BAAI/bge-m3
* Computes cosine similarity between them
* If similarity < 0.65, labels the response as `low_confidence`
* This ensures only semantically relevant evidence is considered trustworthy.


**Step 4: Rule-Based Extraction**

* Implements block-specific rules:

  * E.g., if "net zero" + a year is detected, then `net_zero_target = yes`
  * Adds conditional soft\_yes/yes logic depending on presence of target phrases and context
  * Handles component questions like: “Is there a law?” AND “Is there a target?” for energy efficiency

---

**Handling Negation Intelligently**
Initially, any negation (`neg` dependency) would immediately assign `yes_no = no`. This led to false negatives (e.g., “not yet implemented” or “needs more support”).

**Improvement:** Negation is now detected and preserved, but:

* It does not override a confident `yes` or `soft_yes`
* Instead, we append a soft warning to the explanation:

  * `"(Note: negation present in sentence, which may weaken the claim.)"`

---


**Output Strategy**

* Each country gets a Markdown file. 
* These summaries enable human analysts or auditors to rapidly review decisions per country.

* E.g. the first section of  `argentina_policy_targets.md`, looks like this: 

```markdown

Net Zero Target

- **Answer**: `yes`

- **Explanation**: Mentions net zero target.

- **Year(s)**: 2022, 2050

- **Confidence**: 0.8189

- **Source URL**: https://climateactiontracker.org/countries/argentina/targets/

> Answer/Evidence (Similarity: 0.8251): Further information on how the CAT rates countries (against modelled domestic pathways and fair share) can be found here. **    ## Net zero and other long-term target(s)   We evaluate the net zero target as: Poor. **   In November 2022, Argentina submitted a lon
```

* We can compare the above with the output from running the pure vector search from `climate_tracker/scripts/05_information_retrieval.py`: 
```markdown

Question 1: Does the country have a net zero target, and if so, what year is the target set for?

**Answer/Evidence (Similarity: 0.8251):**
> **Further information on how the CAT rates countries (against modelled domestic pathways and fair share) can be found** **here****. **    ## Net zero and other long-term target(s)   We evaluate the net zero target as: **Poor. **   In November 2022, Argentina submitted a long-term strategy (LTS) to the UNFCCC that includes a target to reach GHG neutrality by 2050 (Government of Argentina, 2022a).

**Source URL:** [https://climateactiontracker.org/countries/argentina/targets/](https://climateactiontracker.org/countries/argentina/targets/)
```

(Note: we can see that the year section does pick up the year of 2022 from the source, as well as the required 2050). 


---

**Why this Order? (NER → Parse → Embed → Rules)**

* NER first filters sentences and assigns semantic tags
* Dependency parsing checks structure and negations
* Cosine similarity confirms if the evidence is semantically close to the question
* Rules then determine final values

> This combination balances generalisability (via embeddings) with control and explainability (via rules).

---

**What does the cossine similarity measure?**

* In 06_policy_extraction.py, cosine similarity is used to validate the relevance of each retrieved chunk by measuring the semantic closeness between the predefined policy question (e.g., "Does the country have a net zero target?") and the retrieved evidence chunk from 05_information_retrieval.py. 

* While 05 initially ranks and selects the top chunk based on similarity, the score is recomputed in 06 from scratch using the same embedding model (BAAI/bge-m3). 

* This recalculation accounts for minor differences caused by tokenizer precision or embedding batch effects. Importantly, this validation step ensures that only semantically relevant chunks proceed to extraction. 

* If the cosine similarity is below a strict threshold of 0.65, the chunk is labeled as low_confidence, signaling that the evidence may be off-topic or too vague to support a definitive answer. This quality control step ensures that our rule-based extraction process only works with evidence that is semantically aligned with the question.

---
**Conclusion**
This script exemplifies a hybrid NLP strategy tailored for climate policy extraction. It combines retrieval validation (via embeddings) with symbolic reasoning (NER, rules, negation) to achieve trustworthy, explainable, and relevant outputs.
